adam_epsilon: 1.0e-08
advanced_search:
  default_importance: 0.5
  gmm_covariance_type: full
  gmm_n_components: 3
  importance_denominator: 4
  importance_width_factor: 2
data_module:
  batch_size: 32
  data_path: data/so_many_rev.csv
  max_length: 128
  model_name: bert-base-uncased
  num_workers: 4
  rating_column: rating
  text_column: text
directories:
  max_length: 100
  model_cache_dir: results/model_cache
  ray_results_dir: results/ray_results
  results_dir: results
hidden_dropout_prob: 0.1
model_architecture:
  pooling_head:
    dense_reduction_factor: 2
    enable_spectral_norm: true
    num_dense_layers: 2
  unfrozen_layers: 3
model_name: bert-base-uncased
network_training:
  fgm:
    default_epsilon: 0.5
    emb_name: embeddings.word_embeddings.weight
  optimizer:
    anneal_strategy: cos
    div_factor: 25
    eps: 1e-8
    final_div_factor: 10000.0
    pct_start: 0.3
  reduce_lr_on_plateau:
    factor: 0.1
    mode: min
    patience: 5
    verbose: true
num_labels: 5
rdrop_alpha: 0.3
scheduler:
  grace_period: 3
  max_t: 20
  metric: val_loss
  mode: min
  reduction_factor: 3
  time_attr: training_iteration
search_space:
  adam_epsilon:
    max: 1.0e-07
    min: 1.0e-09
    type: loguniform
  adv_epsilon:
    max: 0.6
    min: 0.2
    type: uniform
  adv_training:
    type: categorical
    values:
    - true
  attention_probs_dropout_prob:
    max: 0.5
    min: 0.1
    type: uniform
  batch_size:
    type: categorical
    values:
    - 16
    - 32
  fp16_training:
    type: categorical
    values:
    - true
  gradient_accumulation_steps:
    type: categorical
    values:
    - 1
    - 2
    - 4
    - 8
  gradient_checkpointing:
    type: categorical
    values:
    - true
    - false
  hidden_dropout_prob:
    max: 0.7
    min: 0.1
    type: uniform
  hidden_layer_dropout:
    max: 0.7
    min: 0.3
    type: uniform
  initial_learning_rate:
    max: 0.0001
    min: 1.0e-06
    type: loguniform
  intermediate_size_factor:
    type: categorical
    values:
    - 2.0
    - 2.5
    - 3.0
    - 4.0
  label_smoothing:
    max: 0.2
    min: 0.1
    type: uniform
  layer_norm_eps:
    max: 1.0e-05
    min: 1.0e-12
    type: loguniform
  learning_rate:
    max: 0.0001
    min: 1.0e-06
    type: loguniform
  max_grad_norm:
    max: 2.0
    min: 0.5
    type: uniform
  max_lr:
    max: 5.0e-05
    min: 1.0e-05
    type: loguniform
  num_attention_heads:
    type: categorical
    values:
    - 8
    - 12
  rdrop_alpha:
    max: 0.4
    min: 0.2
    type: uniform
  use_mixout:
    type: categorical
    values:
    - false
    - true
  use_swa:
    type: categorical
    values:
    - true
  warmup_steps:
    type: categorical
    values:
    - 100
    - 200
    - 300
    - 500
  weight_decay:
    max: 0.001
    min: 1.0e-06
    type: loguniform
seed: 42
total_steps: 1000
training:
  cpus_per_trial: 1
  early_stopping_patience: 5
  fp16_training: true
  gpus_per_trial: 0.25
  gradient_accumulation_steps: 1
  initial_learning_rate: 1.57e-05
  max_concurrent_trials: 4
  num_samples: 1000
use_tune: true
validation:
  checkpoint_top_k: 1
  log_interval: step
  monitor_metric: val_loss
  monitor_mode: min
wandb:
  project_name: ray_tune
weight_decay: 0.01
